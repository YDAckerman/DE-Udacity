
* Notes for Udacity Data Engineering Nano Degree Program


* Concepts (07/05/22)
  - What is Data Engineering
    - The movement, storage, exploration, and transformation of data.
    
  - What do Data Engineers do?
    - build and manage pipelines and work with stakeholders to
      determine data needs.
      - (side note) I am currently feeling quite drawn to this. It
        sounds like DE's primarily create /internal/ tools with very
        clear deliverables. 
    
  - A Brief history
    - https://medium.com/analytics-and-data/on-the-evolution-of-data-engineering-c5e56d273e37
      ultimate quote: "The role of the data engineer is no longer to
      provide support for analytics purposes but to be the owner of
      data-flows and to be able to serve data both to production and
      for analytics purposes."

  - Data Engineering Tools


* Lesson 1: Intro to Data Modeling

  - What is data modeling?
    - a data model is an abstraction that organizes elements of data and how they will
      relate to each other
    - data modeling is The process of creating data models for an
      information system
    - results in a database system

  - Process overview
    - gather requirements
    - conceptual data modeling
      - entity mapping (think: sales is connected to stores and
        customers, or something like that). each entity will have a
        table, and the tables will be linked in a meaningful way.
    - logical data modeling
      - conceptual models are mapped to logical models, with schemas,
        tables, columes, etc.
    - physical data modeling
      - transform the logical model to the database's DDL (language,
        format, etc.)
      - Data Definition Language

  - Why is data modeling important?
    - well organized data makes everyone's life easier. Queries should
      be natural and as simple as possible.
    - start data modeling early
    - iterative process

  - Who does this type of work?
    - everyone who works with data should understand data modeling

* Intro to Relational Databases
    
    - relational DBs
      - organizes data into one or more tables of columns and rows (or
        tuples), each row having a unique key identifying that
        row. generally, each table represents a type of entity.
      - RDBMS are used to maintain a relational database
      - SQL (structured query language) is the language used across
        most relational database systems for querying and managing.
      - Common relational databases:
        - Oracle
        - Teradata
        - MySql
        - PostgresSQL
        - sqlite

    - Basics

      - Database / Schema
        - collection of tables

      - Tables/Relations
        - group of rows sharing the same labeled elements
          i.e. customers.

      - Columns/Attributes
        - labeled element (name, city, email, etc.)

    - Why to use a relational database

      - ease of use
      - ability to do JOINS
      - ability to do aggregations and analytics
      - effective for smaller data volumes
      - easier to change business requirements
      - flexibility for queries
      - modeling the data, not the queries
      - secondary indexes are available
        - want to efficiently search data by a value that is not your
          primary key.  
      - ACID Transactions - provide data integrity:
        "Properties of database transactions intended to guarantee
        validity even in the event of errors, power failures."
        
        - Atomicity
          "the whole transaction is processed or nothing is processed"
          
        - Consistency
          "only transactions that abide by constraints and rules is
          written into the database, otherwise the database keeps its
          state"
          
        - Isolation
          "transactions are processed independently and securely,
          order does not matter"
          rows lock until queries complete, so you don't get collisions.
          
        - Durability
          "Once a transaction has been COMMITED and is complete it will remain
          committed even in the event of a system failure."

    - When NOT to use a relational DB system

      - Big Data
      - Need to store different types of data formats
      - need high throughput. ACID transactions slow down the
        reading/writing of data.
      - need a flexible schema. these allow columns to be added that
        are not used for every row. Saves space.
      - need *high* *availability*. RDBS are not distributed (if they are,
        they have a coordinator/worker architechture), so in the
        event of failure, a fail-over to a backup system must occur
        which is often costly (in time).
      - Need *horizontal* *scalability*. the ability to add more machines
        or nodes to a system to increase performance and storage.
        (vertical scaling means adding more resources to an individual
        machine - RDBS can do this.)

    - Intro to Postgresql

      An open source object-relational database system.

      - builds on SQL by providing features that reliably store and
        scale complicated workloads.

      - the syntax is different from other sql syntaxes.

      - setup: https://www.codementor.io/@engineerapart/getting-started-with-postgresql-on-mac-osx-are8jcopb

        - Note: postgres doesn't like capitals. need to double quote
          anything with caps. ie. \password Yoni fails. \password
          "Yoni" succeeds.
          
* Intro to NoSql (Not Only SQL)

  - Common nosql dbs:
    - Apache Cassandra (Partition Row Store)
      data is distributed by partition and organized in column/row format
    - MongoDB (Document Store)
      easy to do search on
    - DynamoDB (Key-Value Store)
      data represented as a collection of key-value pairs
    - Apache HBase (Wide Column Store)
      also uses table:column/row format. but has a flexible schema, so
      columns can vary by row
    - Neo4j (Graph Database)
      data represented as nodes and edges

  - Basics of Cassandra

    - Keyspace
      collection of tables
    - Table
      group of partitions
    - Rows
      a single item
    
    - Partition
      - fundamental unit
      - collection of rows
      - how data is distributed
    - Primary Key
      - primary key is made up of a partition key and clustering columns
    - Columns
      - clustering and data
      - labeled element
    
    - Examples of Cassandra use cases:
      - transaction logging
      - internet of things
      - timeseries data
      - any workload that is heavy on write operations.

    - Cassandra is not good for analytics:
      GROUP BY queries are not available (?). data modeling is based
      on the desired query (!), so you can't do ad-hoc
      queries. However, you can add clustering columns into the data
      model and create new tables.

    - Cassandra uses its own query language called CQL

    - When to use NoSQL:
      - need to store different types of file formats
      - large amounts of data
      - need horizontal scalability
      - need high throughput
      - need a flexible schema
      - need high availability
      - users are distributed
        --low latency

    - When not to use NoSQL:
      - small dataset
      - need ACID transations
      - need to be able to do joins
      - want to be able to do aggregations and analytics
      - changing business requirements
      - queries are unavailable and you need the flexibility. i.e. you
        need your queries in advance.
      - (Note) Some nosql dbs do provide some form of ACID
        transations. Mongodb added the ability to do so in some
        contexts in v4.0 and v4.2. Another is MarkLogic.

    - Installing Cassandra:
      https://cassandra.apache.org/doc/latest/cassandra/getting_started/installing.html

    - Figuring this out took me 2 hours...
      - You need to run:
        - docker run --name <some name> --network <network name> -d
          cassandra:latest
        - I used network 'bridge'
      - Then make sure you have a python:3.8-slim image (which I
        needed to make a docker login to get (?)) and run:
        - docker run -it --rm --network <network name> python:3.8-slim
          bash
        - pip install cassandra-driver
      - Then within a python terminal:
        - from cassandra.cluster import Cluster
          cluster = Cluster(['172.17.0.2'], port=9042)
        - The stackoverflow answer I used told me to use the container
          name 'some-cassandra' but that didn't work. so I looked up
          some-cassandra's machines IP using:
          - docker network inspect bridge
      - I think the next step here is to create a DOCKERFILE that will
        set this up automatically.

    - Ok things are definitely working and I can make a keyspace. But
      before adding tables to the keyspace, I need to know the queries
      I'm going to make. In the demo, the query I'm going
      to run is "select * from music_library where year == 1970". That
      means 'year' needs to be my partition key and the artist name
      will be the clustering column. THERE ARE NO DUPLICATES IN APACHE
      CASSANDRA. so: PRIMARY KEY(year, artist_name)
        
* Lesson 2: Relational Data Models (7/8/22)

  - Definitions:

    - *Database*:
      a set of related data and the way it is organized.
    - *Database Management System*:
      The software that allows you to interact with a database.

  - *History*:

    - RDBs were invented in the 1969 by researchers are IBM. The lead
      researcher proposed 12 rules defining RDBs.
    
  - Rule 1 (the information rule):

    - all information in a relational database is represented
      explicitly at the logical level and in exactly one way: by
      values in tables.

  - Why is the Relational Model important?

    - standardization of the data model
    - Flexibility in adding and altering tables
    - Data Integrity
    - SQL
    - Simplicity
    - Intuitive organization

  - *OLAP*:
    - Online Analytic Processing
    - Emphasis on the Analytic
    - An OLAP-optimized DB is optimized for complex analytical and
      ad-hoc queries, including aggregations. They are optimized for
      reads.

  - *OLTP*:
    - Online Transactional Processing
    - Emphasis on the Transactional
    - An OLTP-optimized DB is optimized for less-complicated queries
      in large volumes. queries are read, insert, update, and delete.

  - Structuring The DB:
    
    - Normalization:
      - The process of structuring a relational database in accordance
        with a series of *normal forms* in order to:
      - To reduce data redundancy and increase data integrity
        
    - Denormalization:
      must be done in read-heavy workloads to improve performance

    - Purposes of Normal Forms:
      - To free the db from unwanted insertions, updates, and deletion
        dependencies (ideally only update in one place.)
      - To reduce the need for refactoring the database as new types
        of data are introduced
      - to make the relational model more informative to users
      - to make the database neutral to the query statistics.

    - First Normal Form (1NF):
      - create atomic values. Each cell has unique values. No lists or
        sets in columns.
      - be able to add data without altering tables.
      - separate different relations into different tables (customers
        and sales).
      - keep relationships between tables together with foreign keys
        
    - Second Normal Form (2NF):
      - reach 1NF
      - All columns must rely on the primary key. should not need two
        elements to get a third. 

    - Third Normal Form (3NF):
      - reach 2NF
      - No transitive dependencies. reaching a value should not
        depend on values that the desires value is not logically
        dependent on. so you shouldn't have to know an award and year
        the award was issued to look up the lead singer of the band
        the award was given to.
      - When you want to update data, you want to be able to do it in
        just one place.

    - *Denormalization* (again):
      - The process of improving the read performance of a database at
      the expense of losing some write performance by adding redundant
      copies of the data.
      - comes after normailzation.
      - requires more space.

    - *Normalization* is about increasing data integrity by reducing
      the number of copies of the data. Data that needs to be added or
      updated will be done in as few places as possible.

    - *Denormalization* is increasing the performance of reads by
      reducing the number of joines between tables. Data integrity
      will take a bit of a hit, as there will be more copies of the
      data.

    - *Fact* and *Dimension* tables
      - Work together to form an organized data model
      - Not created differently from the DDL perspective,
        they are conceptual, and important from an organizational
        standpoint.
    - Fact tables:
      consist of measurements, metrics, or facts of a business
      process. facts: "Events that have actually happened"
    - Dimension tables:
      a structure that categorizes facts and measures in order to
      enable users to answer business questions. Dimensions are often
      people, products, place, and time.
      
    - Different Schemas:
      Two of the most popular (because of simplicity) data mart
      schemas for data warehouses are:

      - Star Schema

        What:
        1. Simplest data mart schema. Consists of one or more fact
           tables referencing any number of dimension tables
        2. ERD resembles a star
        3. fact table at center with dimension tables surrounding it.
        
        Why:
        1. 3NF is a lot of work and joins can be complex.
        2. star schemas allow for relaxing of the rules (are
           denomalized) and make queries easier.
        3. aggregations perform calculations and clustering of the
           data, so the application doesn't have to.

        Cons:
        1. issues with denormalization
        2. data integrity
        3. decreased query flexibility
        4. many to many relationships need to be simplified.

      - Snowflake Schema

        What:
        1. more general class of shema, of which star is a special
           case.
        2. allows for one-to-many relationships
        3. more normalized than star, but only in 1NF or 2NF
        
* SQL NOTES

  - CREATE TABLE
    - NOT NULL
      1. ex:
         CREATE TABLE IF NOT EXISTS customers (
         customer_id int NOT NULL,
         store_id int
         );
      2. ex (composite key):
         CREATE TABLE IF NOT EXISTS customers (
         customer_id int NOT NULL,
         store_id int NOT NULL,
         spent numeric
         );
    - UNIQUE
      1. ex:
         CREATE TABLE IF NOT EXISTS customers (
         customer_id int NOT NULL UNIQUE,
         store_id int NOT NULL UNIQUE,
         spent numeric
         );
      2. ex (table constraint):
         CREATE TABLE IF NOT EXISTS customers (
         customer_id int NOT NULL,
         store_id int NOT NULL,
         spent numeric,
         UNIQUE (customer_id, store_id)
         );
    - PRIMARY KEY
      1. ex:
         CREATE TABLE IF NOT EXISTS store (
         store_id int PRIMARY KEY,
         store_location_city varchar,
         store_location_state varchar
         );
      2. ex (table constraint):
         CREATE TABLE IF NOT EXISTS customers (
         customer_id int,
         store_id int,
         spent numeric,
         PRIMARY KEY (customer_id, store_id)
         );
  - UPSERT
    *updating or inserting*
    1. ex (ON CONFLICT DO NOTHING):
       CREATE TABLE IF NOT EXISTS customer_address (
       customer_id int PRIMARY KEY,
       customer_street varchar NOT NULL,
       customer_city text NOT NULL,
       customer_state text NOT NULL
       );

       INSERT INTO customer_address (customer_id, customer_street,
       customer_city, customer_state) VALUES (432, '758 Main St.',
       'Berkeley', 'CA');

       INSERT INTO customer_address (customer_id, customer_street,
       customer_city, customer_state) VALUES (432, '758 Main St.',
       'Chicago', 'IL')
       ON CONFLICT (customer_id)
       DO NOTHING;

    2. ex (ON CONFLICT DO UPDATE):
       INSERT INTO customer_address (customer_id, customer_street)
       VALUES (432, '748 Main St.')
       ON CONFLICT (customer_id)
       DO UPDATE
       SET customer_street = EXCLUDED.customer_street;

* LESSON 3: NoSQL databases

  - Basics
  - Denormalization
  - Primary keys
  - Clustering columns
  - the WHERE clause

  - Non-relational but also Not Only SQL
  
  - When to use NOSQL
    - need high availability - system is always up
    - large amounts of data
    - need linear scalability - need to add more nodes to the system
      so the performance increases linearly
    - low latency: shorter delay before the data is transferred once
      the instruction for the transfer has been received
    - need fast reads and writes.

  - apache cassandra
    - open source
    - masterless architecture
    - highly available
    - linearly scalable
    - often used for apps

  - Node <=> Server

* Basics

  - Distributed database - resilient to node failure.
    - copies of your data may not be up to date in all locations -
      this is *Eventual Consistency*
  - Eventual Consistency:
    informally guarantees that, if no new updates are made to a given
    data item, eventually all accesses to that item will return the
    last updated value.

  - *CAP Theorem*: it is impossible for a distributed data store to
    provide more than two of the following three guarantees:

    - Consistency: every read gets the latest (and correct) piece of
      information OR returns an error.

    - Availability: Every request is received and a response is given
      (without a guarantee that the result is the most up-to-date)

    - Partition Tolerance: the system continues to function even if
      connectivity is lost between nodes

  - ACID vs CAP

* NoSQL Data Models
  Note: statements are made in the context of apache cassandra

  - Denormalization is a must (for fast reads)
  - Cassandra is optimized for fast writes
  - ALWAYS think, "queries first"
  - One table per query is a great strategy
  - cassandra does not allow for joins

* Cassandra Query Language

  - does not support:
    - JOIN
    - GROUP BY
    - Subqueries

  - Has many language drivers. We'll be using Python's


* Cassandra PRIMARY KEY

  ex:

  CREATE TABLE IF NOT EXISTS cars (year int, make text, model text,
  color text, PRIMARY KEY ((make, model), year))

  - must be unique.
  - can be simple or composite.
  - made up of the partition key and any number of optional clustering
    columns. The partition key determines the distribution of the data
    across the system. The partition key can be simple or composite.
  - it is desireable to pick a key that will evenly distribute the
    data. i.e if your key is state, the data won't be distributed
    evenly because there are so many people in california than in vermont.
  - the partition key for a given row is hashed and stored on the node
    in the system assigned that range of hashed values.


  - partition key vs clustering columns. clustering columns sort
    results within partition in ascending order.

* WHERE Clause

  - WHERE must be included
  - The Partition Key must be included in the query.
    Any Clustering Columns must be used in the order they appear in
    the Primary Key.
  - WHERE allows for fast reads.


* INTRODUCTION TO DATA WAREHOUSES

  * What is a data warehouse?
    - business perspective:
      (a) There are two business processes: opertaional (make it work!)
      and analytical processes (what's going on?).
      (b) operational databases aren't necessarily good for
      analytics.
      (c) OLTP (transaction processing) vs OLAP (analytical processing)
    - A Data Warehouse is a system that enables us to support
      analytical processes.

  * Three definitions:
    - copy of transaction data specifically structured for query and
      analysis
    - subject-oriented, integrated, nonvolatile, and time-variant
      collection of data in support of management's decisions.
    - a system that retrieves and consolidates data periodically from
      the source systems into a dimensional or normalized data
      store. it usually keeps years of history and is queried for
      business intelligence or other analystical activities. It is
      typically updated in batches, not every time a transaction
      happens in the source system.

  * Dimensional Model Review

    - Star Schema: fact and dimension tables.
      (a) Fact tables record events relevant to the business. contains
      quantifiable metrics: quantity of an item, duration of a call,
      book rating, etc. A good candidate for a fact is numeric and additive.
      (b) Dimension tables provide contexts and attributes for the
      Fact tables. item purchased, customer who made the call, etc.

  * Naive etl: moving from 3NF to Star
    - extract from 3NF
    - transfrom in the form of joins, type changes, and new columns
    - load by inserting into fact and dimension tables.

  * DWH Architectures:
    - Kimball's Bus
    - Independent Data Marts
    - Inmon's Corporate Information Factory (CIF)
    - Hybrid Bus & CIF

  * Kimball's Bus:
    - Source Transactions then:
    - ETL System then:
    - Presentation Area:
      - Dimensional
      - Atomic and summary Data
      - organized by business processes
      - uses conformed dimensions (generalize dimensions so that all
        dimensions are usable by the whole organization)
    - Results in a bus Matrix (where things are)
    - ETL:
      - extracting
        - get data from source
        - possibly deleting old state
      - transforming
        - integrates many sources together
        - possible cleaning
        - possible meta diagnostics
      - Loading
        - structuring and loading into a dimensional model

  * Independent Data Marts
    - Each department has separate ETL processes
    - there is work and data repitition
    - can give uncoordinated and inconsistent views
    - the advantage is departmental autonomy, avoids coordination
      overhead.

  * Inmon's Corporate Information Factory (CIF)
    - Data acquisition then:
    - (ETL) enterprise dataware house (3nf Database) then:
      - single integrated source of truth for data marts
      - can be accessed by end-users if necessary
    - (ETL) data delivery to:
    - data marts (now have a 3nf baseline)
      - dimensionally modelled and (unlike Kimball's) mostly
        aggregated.

  * Hybrid Kimball Bus & Inmon CIF
    - Removes Data Marts
    - Exposes the enterpries DW bus architecture

  * OLAP Cubes

    - From a star schema, create OLAP cubes
    - These are tables aggregating a number of facts/dimensions. For example
      Tables for Movie, Revenue, Branch/Location, Month (1 table for each month)
    - These are very easy for business users

    - *Rolling Up and Drilling Down*:
      - aggregating and disaggregating on specific dimensions. I.e sum
        of sales for each country (summing over city). or decomposing
        cities into smaller districts.

    - OLAP cubes should store data at the finest granularity.

    - *Slicing and Dicing*:
      - slices are essentially marginal distributions.
      - dices are sub-cubes created by restricting the range of
        certain values.

    - OLAP Cubes Query Optimization
      - There is a GROUP by CUBING statements:
        - GROUP BY SETS
        - GROUP BY CUBE
      - When you call CUBE, you do one pass through the facts
        table. This gives you all combinations of, for example,
        (movie, branch, month).
      - Saving all the combinations, is usually sufficient for a very
        wide number of future queries.

    - Links:
      - https://www.amazon.com/Data-Warehouse-Toolkit-Complete-Dimensional/dp/0471200247

      - https://www.amazon.com/Building-Data-Warehouse-W-Inmon/dp/0764599445

      - https://www.amazon.com/Building-Data-Warehouse-Examples-Experts/dp/1590599314

    - OLAP Cube Technologies:
      - MOLAP (multidimensional online analytical processing)
        - cubes are stored on a dedicated server using a
          non-relational database structure.
      - ROLAP (Relational online analytical processing)
        - cubes are created on the fly and stored in relational
          databases

* AWS

  - Create an IAM role
    - select entity (AWS service)
    - select use case (Redshift - Customizable)
    - Attach permissions policies: check AmazonS3ReadOnlyAccess
    - add role name (MyRedshiftRole)
      
  - Create Security Group
    - Navigate to EC2 services
    - open security groups
    - create new security group
      - group name: redshift_security_group
      - description: Authorize Redshift Cluster Access
      - VPC: default
      - Inbound rules:
        - add new:
          - Type: Custom TCP
          - Protocol: TCP
          - Port Range: 5439 (Default)
          - source: 0.0.0.0/0 (anywhere in the world)
            (note: for demonstration purposes only. this allows access
            from any computer on the internet!)
      - Outbound rules:
        - default (allow traffic to anywhere)
          
  - Launch a Redshift Cluster
    - Create Cluster:
      - unique id: redshift-cluster-1
      - choose free trial.
      - cluster specs (default):
        - dc2.large
        - 2 vCPUs
        - 160 GB storage
      - username and pw created and stored.
    - (Note: in free tier the following can be configured once the
      cluster is available)
    - Associate IAM role MyRedshiftRole
    - Network and Security:
      - VPC: default
      - VPC Security Groups: redshift-security-group (and default?)
    - Subnet Groups
      - create cluster subnet group and select in network/security
        configuration (I wasn't able to do this...)
        
  - Create IAM User
    - give username
    - allow programmatic access
    - attach existing policies:
      - RedshiftFullAccess
      - S3ReadOnly
    - review and download credentials (.csv)
      
  - S3
    - (unique) name
    - allow public access
    - disable versioning and encryption
      
  - Amazon RDS
    - create database
    - postgresql - latest version is default
    - free tier template
    - postgreSQL-test identifier
    - master username and password stored
    - default storage
    - default connectivity with:
      - public access YES (choose no when more familiar)
      - redshift_security_group and default
    - provide a db name in additional configuration (testdb)

* DWH on AWS
  
  - Cloud Managed:
    - Amazon RDS, DynamoDB, S3
    - re-use of expertise, less staff, etc.
    - deal with complexity through 'infrastructure as code'

  - Self Managed:
    - EC2 + {postgresql, cassandra, unix}
    - essentially have ec2 do everything

  - Redshift:
    
    - column-oriented storage
    - best oriented for OLAP workloads.
    - internally, it's a modified postgresql

    - most relational databases execute queries in parallel, with each
      cpu executing a single query. This is good for OLTP, as you may
      have many users executing small updates and retrievals.

    - Massively parallel processing (MPP) parallelizes the execution
      of a single query on multiple cpus/machines.
    - This is done via table partitioning.

    - Redshift is cloud-managed, column-oriented, and MPP.

    - Architecture:
      - Leader Node:
        - coordinates compute nodes
        - handles external communication
        - optimizes query execution
      - Compute Nodes
        - each has it's own cpu, memory, and disk
        - these can be scaled up or scaled out
        - each node is divided into a number of slices: a cluster with
          n slices can process n partitions of a table simultaneously.
          - a slice has atleast one cpu and dedicated storage and memory
          - sum of all slices across all compute nodes is the unit of
            parallelization.
      - Nodes can be optimized for compute or storage. these come with
        cost/benefit pros/cons.
      - The coming demo with be 4 nodes at $.25/hour! *SET UP BILLING ALERTS*

  - SQL-TO-SQL ETL
    
    - we will want to copy the results of a query to another table on
      a totally different database server.
    - this is feasible when both db servers are running the same
      RDBMS, but much harder if different.
    - Either way, there will likely be some transforming, cleaning,
      and or governance that needs to be done along the way.

    - with an EC2 instance we can direct one db to transfer (COPY) to s3, and
      then another db to query that s3 bucket. This bypasses the need
      for local storage on EC2.

    - FLOW IN CONTEXT:
      - sources:
        - s3
        - cassandra
        - ec2
        - dynamodb
        - etc.
      - ETL:
        - ec2
        - aws data pipeline
        - airflow
      - STAGING:
        -s3
      - DWH:
        - redshift
      - OLAP CUBES:
        - s3
        - aws rds
        - cassandra
      - BI Apps
        - jupyter
        - tableau

    - Transferring data from s3 staging to redshift
      - use the COPY command
      - may be better to break into multiple files and ingest in
        parallel:
        - use a common prefix
        - a manifest file
      - better to ingest from the same aws region
      - better to compress all the csv files

    - COPY (PREFIX) example
      #+BEGIN_SRC sql
        COPY sporting_event_ticket FROM 's3://udacity-labs/tickets/split/part'
        CREDENTIALS 'aws_iam_role=arn:aws:iam::464956546:role/dwhRole'
        gzip DELIMETER ';' REGION 'us-west-2';
      #+END_SRC
      'part' can just be a prefix matching multiple files.

    - COPY (MANIFEST) example:
      #+BEGIN_SRC sql
      COPY customer
      FROM 's3://mybucket/cust.manifest'
      IAM_ROLE 'arn:aws:iam::01234567890:role/myRedshiftRole'
      manifest;
      #+END_SRC sql
      with cust.manifest:
      {
      'entries': [
      {"url": "s3://mybucket-alpha/2013-10-04-custdata", "mandatory" :
      true},
      {"url": "s3://mybucket-alpha/2013-10-05-custdata", "mandatory" :
      true},
      {"url": "s3://mybucket-alpha/2013-10-06-custdata", "mandatory" :
      true}
      ]
      }

    - compression
      - redshift gives control over each column's compression. (see
        redshift documentation)
      - COPY makes a best effort to compress each column optimally

    - ETL from other sources
      - possible to ingest directly from ssh from an EC2 machine's
        storage
      - otherwise:
        - s3 is a *staging area*
        - so some other EC2 ETL working needs to run the ingestion
          jobs via some *orchestarted dataflow product* like airflow,
          luigi, nifi, streamset, or aws data pipeline.

    - ETL *OUT* of redshift
      - redshift can connect directly to BI Apps via (JDBC/ODBC)
      - may want to extract pre-aggregated OLAP cubes:
        #+BEGIN_SRC sql
        UNLOAD ('select * from venue limit 10')
        to 's3://mybucket/venue_pipe_'
        iam_role 'arn:aws:iam:012345:role/MyRedshiftRole';
        #+END_SRC sql
        (in this case read credentials are required)

  - Building a Redshift Cluster
    (see above, there is no longer a quick-launch option. Plus I don't
    think this example is explicit enough: the free-trial version
    doesn't allow you to change the number of nodes. If it does, it
    must be post startup.)

  - Issues:
    - the previous cluster is operational, but we need additional
      functionality:
      - Security:
        - needs to be accessible *ONLY* from the VPC
        - need to access it from our jupyter workspace.

      - Access to s3:
        - the cluster needs (read) access to an s3 bucket.

    - Redshift needs an IAM role.
    - Chage the TCP ports open in the security group

  - Modern engineering uses *Infrastructure as Code*
    
    - "creating a machine is as easy as opening a file"

    - aws-cli
    - aws sdk (python, java, etc.)
    - amazon cloud formation (jscon description)
      - atomic (everything succeeds or nothing does)

    - we will proceed with aws sdk for python: boto3
    - we'll create an IAM user dwhadmin and give it admin privileges
      (this is, in general excessive, but helpful for the demo)
    - user gets an access token and secret.
      
      
  - Optimizing Table Design
    - table partitioning is done blindly
    - with knowledge of common access patterns, partitioning can be
      done intelligently
    - strategies:
      - distribution style
        - EVEN, ALL, AUTO, KEY
      - sorting key

    - EVEN distribution:
      - Round-robin over all slices to achieve even load balancing
        (M rows evenly distributed among N slices)
      - good if there won't be joins:
        data will need to be copied from one slice onto another before
        everything is combined in the final output. This process is
        called *Shuffling*. Joins can't be done in parallel.
    - ALL distribution:
      - storing dimension tables in this way insures super efficient
        joins with no shuffling. (when fact is distributed EVEN).
        Known as *Broadcasting*. There tend to be fewer rows in
        dimension tables relative to fact tables.
    - AUTO:
      - leaves the decision to redshift. 'small enough' tables are
        broadcast while large tables are EVENly distributed.
    - KEY:
      - fact tables are distributed by a foreign dim key. This puts
        all data with a specific key on one slice. The distribution
        can end up skewed.
      - 'distkey'
      - the dim table is then distributed accordingly to allow for
        parallel joins. (i.e. one key per slice.)
    - Sorting Key
      - defining a column as a sort key allows:
        - rows to be sorted before distribution
        - minimizes query time
        - useful in columns frequently used for sorting, i.e. date.

* Project 2 (notes):

  - IAC steps:
    
    - setup configuration file:
      - key, secret
      - cluster type
      - num nodes
      - node types
      - cluster id
      - db name
      - db user
      - db pw
      - db port
      - IAM role name

    - initiate boto3 resources:
      - ec2 (for sg)
      - s3

    - initiate boto3 clients:
      - iam
      - redshift

    - (checkout s3 bucket)

    - create IAM role

    - attach policy to role:
      - AmazonS3ReadOnlyAccess

    - get role ARN

    - create redshift cluster using boto3 client
      - cluster type
      - node type
      - num nodes
      - db name
      - cluster id
      - db user name
      - db pw
      - role arn

    - record DWH endpoint and arn

    - open TCP port to access the cluster endpoint. (this is done via
      ec2 vpc)
      
    - connect to the postgres db. do things.
      
       
* Lesson 5: Spark and Data Lakes

  - What is big data? Requires the use of multiple machines, usually
    in the cloud.
  - Four Numbers Everyone Should Know:
    - How long does it take for your CPU to add two numbers?
    - How quickly can you look up an appointment if your calendar is
      already cached in you laptop's memory?
    - How many seconds does it take to load your favorite song from
      your laptop's SSD storage?
    - How much data can you download from Netflix in a minute?

  - CPU:
    - "the brain". does computer operations. 1 cpu operation takes .4 ns
      (for the typical computer)
    - directs other components and runs mathematical calculations.
    - can store small amounts of data in /registers/ - these hold data
      that the cpu is currently working on. registers make
      computations more efficient, as data doesn't (necessarily) have to be sent
      back and forth between memory and the CPU.
  - Memory:
    - "scratch paper" helps with more complicated computations. One memory reference takes 100 ns (for the
      typical computer)
    - efficient
    - ephemeral
    - expensive
  - Storage:
    - "file cabinet". Random read from SSD: 16us.
    - approximately 15% slower than memory.
  - Network:
    - Access to the outside world. Round trip for data (?) from EU to
      US: 150 ms.
    - most common bottleneck when working with big data.
    - 20x when data must be downloaded from another machine.
    - Distributed systems try to minimize shuffling data back and
      forth across different computers for this reason.

  - Shorthand:
    - CPU is 200x faster than memory
    - Memory is 15x faster than SSD
    - SSD is 20x fater than network


* History of distributed and parallel computing

  - network topology: a graph with each node having it's own memory
    and cpu
  - parallel computing: all processors may have access to shared
    memory to exchange information.
  - distributed computing: nodes (cpu + memory) are connected across a
    network.

* Hadoop Vocabulary

  - Hadoop: an ecosystem of tools for big data storage and data
    analysis
  - Haddop MapReduce: system for processing and analyzing large
    datasets in parallel
  - Hadoop YARN: a resource manager that schedules jobs across a
    cluster
  - Hadoop Distributed File System (HDFS): a big data distributed
    storage system.
  - Apache Pig: a SQL-like language that runs on top of hadoop
    mapreduce
  - Apache Hive: another SQL-like language that runs on top of hadoop

  - Basic Differences with Spark:
    - Hadoop writes intermediate results to disk, whereas Spark tries to
    keep them in memory.
    - Hadoop uses HDFS to store data, whereas Spark can read in data
      from other sources.

  - Streaming:
    - Spark not so popular
    - More often Storm and Flink, as they can deliver results in ms.

* MapReduce

  - Each Datum is reduced to a key/value pair, key/value pairs are
    shuffled across the network, and results are aggregated for each
    key.

* Cluster Configuration

  - Spark modes
    - Local Mode:
      - learn syntax
      - prototype project
    - Standalone
      - Interact with Driver Process via Python or Scala
    - YARN (good for sharing a cluster with at team)
    - Mesos (good for sharing a cluster with at team)

* Spark Use Cases

  - Data Analytics
  - Machine Learning:
    - page rank
    - logistic regression
  - Streaming (not used so much)
  - Graph Analytics
  - ETL

* Spark Alternatives

  You don't need Spark for "small data"

  - AWK
  - R
  - Python
  - MySQL / PostgreSQL
  - TensorFlow / PyTorch

  are all better when working on data that can fit, or nearly fit, on
  a single machine.

  Some alternatives to storing and processing distributed data that
  use SQL-like querying are:
  
  - HBase
  - Cassandra
  - Impala
  - Presto

  Spark is NOT a data storage system. Whereas the aforementioned ARE.

* Spark Limitations

  - Streaming latency is 500 milliseconds. Other tools such as Storm,
    Apex, or Flink (which are built for streaming) can out compete
    spark for low-latency applications.

  - Selection of machine learning algorithms
    - only algorithms that scale linearly with input data size are
      supported by Spark.

* Hadoop vs Spark

  - Hadoop is older technology, and despite being slower in some use
    cases, is simply what some early 'Big Data' adopters use.

  - Spark is not significantly faster than Hadoop for some simple
    operations, such as counting. In some use cases, migrating legacy
    code is not cost effective.

* Data Wrangling with Spark

  - functional programming is good for distributed systems
    - a function in mathematics has a much stricter definition that in
      general programming. For example, it doesn't even make sense to
      talk about mathematical functions 'changing' the input
      data. Also, mathematical functions are constrained by their
      domain (set of possible inputs) and range (set of possible
      outputs). Not only does the function output have to reside in
      the range, functions must also output a single, unique element
      of that range.
    - an important feature of functional programming is the lack of
      side effects. That is, functions cannot have any effects on
      information outside their scope. this makes calculations independent of
      one another and insures they do not alter the original data they
      operate on.
  - Call functions that adhere to these principles 'pure' functions.
  - Spark makes copies of it's input data, leaving the original data
    unchanged.
  - functions are composed together in chains, and overall directions
    take the form of directed acyclic graphs (DAGs)
  - spark uses a concept called lazy evaluation - no computations are
    actually made until necessary. This avoids memory issues.
  - multi-step operations in the lazy evaluation sheme are called stages.

  - fundamental tools:
    - parallelize
    - maps
    - lambda
    - collect

  - data formats
    - CSV, JSON, HTML, XML

  - Big data usually means distributed data.
    Distributed big data is designed to tolerate the loss or
    disruption of individual machines.

  - SparkSession
    - *SparkContext* is the main entry point to Spark functionality and
      connects the cluster with the application
    - *SparkConf* is used to configure the connection, i.e. specifying
      name and master node's IP address. in local mode, specify
      'local' as master
    - *SparkSession* is 'context' used to read data frames
    - *getOrCreate()* is SparkSession SparkConf equivalent, can also
      get any SparkSession currently running.

      #+BEGIN_SRC python
        from pyspark import SparkContext, SparkConf
        configure = SparkConf().setAppName("name").setMaster("IP Addres") # or local
        sc = SparkContext(conf = configure)
      #+END_SRC

      #+BEGIN_SRC python
        from pyspark.sql import SparkSession
        spark = SparkSession.builder.appName("name").config("option", "value").getOrCreate()
      #+END_SRC

    - Spark I/O
      - a common error is attempting to use input data that not
        all the worker nodes have access to. An exception like this will
        arise:

      #+BEGIN_SRC python
        AnalysisException: u'Path does not exist: file:/home/ubuntu/test.csv;'
      #+END_SRC

    - Imperitive vs declarative programming:
      - imperitive tells the computer how to do something
      - declarative tells the computer what to get/achieve.
      - there is often a declarative layer that abstracts away the
        imperative instructions. 

    - Data Manipulation:
      - General:
        - select: returns new df with selected cols
        - filter: filter rows by condition
        - where: aka filter
        - groupBy: groups data for aggregation
        - sort: returns sorted df, ascending by default
        - dropDuplicates: drops duplicates
        - withColumn: returns a new df with with added/replaced
          column.

      - Aggregate:
        - count
        - countDistinct
        - avg
        - max
        - min
        - groupBy().agg({"X": "avg", "Y": "max"})

      - user defined functions UDF
        - create user defined functions
          - spark.sql.functions
          - spark.sql.types

      - Window Functions
        - combine values from a range of rows in a df.
        - group with partitionBy
        - rangeBetween or rowsBetween to get window width

    - Spark SQL
      - createOrReplaceTempView

    - python and sql based spark interactions are translated into a
      lower lever DAG execution plan via an optimizer called Catalyst.

    - RDD: Resilient Distributed Data Set

    - Spark and Data Lakes

      - Local Mode -> Cluster Mode

        - Standalone, Mesos, Yarn

        - mesos and yarn are for teams.

      - s3 <-> spark cluster on ec2

        - Options:
          - EC2 and self setup
          - EMR (elastic map reduce) which is preconfigured for spark

      - *AWS CLI PROFILE NAME* udacity_de_l5
        - you can reconfigure options using:
          aws configure set <varname> <value> [--profile profile-name]

      - (note on Instances):
        - General purpose
        - Compute optimized
        - Memory optimized
        - storage optimized

    - Spark Scripts and Spark Jobs
      spark-submit script.py

    - s3 vs hdfs
      - s3 is easier, but may have to carry data farther over the
        network
      - hdfs requires user maintainance, but keeps data closer to
        computations.
      - hdfs has limited file format options. s3 is object storage.
      - hdfs is a true distributed file system and guarantees fault
        tolerance.

    - *writing files to hdfs*

      - hdfs dfs -mkdir /user/sparkify_data
      - hdfs dfs -copyFromLocal <file> /user/sparkify_data

    - *reading files from hdfs*
      
      - path = "hdfs:///user/sparkify_data/<file>"

    - Debugging Spark
      - syntax errors
        - method name errors are shorthand
        - incorrect column names are long analysis exception errors.
        - variable typos can result in lengthy errors
        - when collecting take care
        - close parens properly... or EOF

      - data errors
        - spark populates malformed fields with nulls
        - logs3.where(logs3("_corrupt_record").isNotNull()).collect()
          for example, finds corrupted records.

      - Accumulators
(https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html#accumulators)
        - special variables. 
        - act like global variables for the entire cluster.
        - SparkContext.accumulator(0,0) etc

      - Transformations and Actions:
        - action functions trigger evaluation
        - transformation functions do not.

      - Spark WebUI
        - helps you understand what's going on in your cluster
        - provides:
          - cluster configuration
          - DAG
            - Stages
              - tasks
          - profiling jobs
        - connecting:
          - port 8888 for jupyter
          - 4040 shows active spark jobs
          - 8080 to access webUI

      - Logging
        (see https://spark.apache.org/docs/latest/configuration.html)
        - sparkContext.setLogLevel("ERROR")
        - sparkContext.setLogLevel("INFO")

      - Code Optimization (for big datasets)
        - data skew:
          data is not distributed evenly across the cluster.
          - Pareto Principle:
            80% of data from 20% of users.

        - divy up data by a different field.
            
        - data summaries will help!

        - df.repartition(<number of workers>)


* Data Lakes
  - needed to accommodate changes in incoming data
  - not a replacement for relational databases
  - some data are hard to put into tabular format
  - accommodate the need to explore and analyze unstructured data,
    especially in the context of data science and machine learning.
    
  - Shema on Read:
    - StructType(()) to specify schema
    - spark.read.csv(inferSchema {T/F})
      - give explicit schema
      - tell it what to do with malformed data
    - spark can read a lot of data types:
      - csv
      - avro (binary)
      - parquet (columnar)
      - gzip and snappy (compressed)
    - can read/write from cloud
    - can read/write from database
      - sql through jdbc
      - nosql: mongodb, cassandra, neo4j, etc.

  - Ideas:
    - data of all types, formats, structures, values
    - not ETL, but ELT:
      - data is transformed later (schema on read)
    - distributed data storage out of the box.

  - How to Build a Data Lake on AWS
    - storage:
      - HDFS
      - S3
    - processing:
      - Spark, Hive, Flink
    - AWS managed:
      - EMR
        - HDFS + Spark
        - Spark only
      - Athena (serverless)
    - Vendor Managed:
      - EC2 + Vendor solution

  - AWS EMR (HDFS + Spark)
    - queried in place
    - only results come to the analyst
    - cluster must be available to access data
    - pay the cost of the ec2 machines on which the emr is running
      (these are on 24/7)

  - AWS EMR (s3 + Spark)
    - s3 is the lake storage
    - emr cluster only has spark
    - load data over the network and computer on emr
    - cluster does not need to be on 24/7, only while computations are
      running.
    - may be cheaper, but performance is lower than HDFS
      solution. However network speed on the cloud has improved
      dramatically

  - Athena
    - serverless. I haven't heard of anyone using this.
      
  - Issues:
    - can be chaotic.
    - data governance can be tricky
    - how it fits in with / replaces a data warehouse.
