
* Notes for Udacity Data Engineering Nano Degree Program


* Concepts (07/05/22)
  - What is Data Engineering
    - The movement, storage, exploration, and transformation of data.
    
  - What do Data Engineers do?
    - build and manage pipelines and work with stakeholders to
      determine data needs.
      - (side note) I am currently feeling quite drawn to this. It
        sounds like DE's primarily create /internal/ tools with very
        clear deliverables. 
    
  - A Brief history
    - https://medium.com/analytics-and-data/on-the-evolution-of-data-engineering-c5e56d273e37
      ultimate quote: "The role of the data engineer is no longer to
      provide support for analytics purposes but to be the owner of
      data-flows and to be able to serve data both to production and
      for analytics purposes."

  - Data Engineering Tools


* Lesson 1: Intro to Data Modeling

  - What is data modeling?
    - a data model is an abstraction that organizes elements of data and how they will
      relate to each other
    - data modeling is The process of creating data models for an
      information system
    - results in a database system

  - Process overview
    - gather requirements
    - conceptual data modeling
      - entity mapping (think: sales is connected to stores and
        customers, or something like that). each entity will have a
        table, and the tables will be linked in a meaningful way.
    - logical data modeling
      - conceptual models are mapped to logical models, with schemas,
        tables, columes, etc.
    - physical data modeling
      - transform the logical model to the database's DDL (language,
        format, etc.)
      - Data Definition Language

  - Why is data modeling important?
    - well organized data makes everyone's life easier. Queries should
      be natural and as simple as possible.
    - start data modeling early
    - iterative process

  - Who does this type of work?
    - everyone who works with data should understand data modeling

* Intro to Relational Databases
    
    - relational DBs
      - organizes data into one or more tables of columns and rows (or
        tuples), each row having a unique key identifying that
        row. generally, each table represents a type of entity.
      - RDBMS are used to maintain a relational database
      - SQL (structured query language) is the language used across
        most relational database systems for querying and managing.
      - Common relational databases:
        - Oracle
        - Teradata
        - MySql
        - PostgresSQL
        - sqlite

    - Basics

      - Database / Schema
        - collection of tables

      - Tables/Relations
        - group of rows sharing the same labeled elements
          i.e. customers.

      - Columns/Attributes
        - labeled element (name, city, email, etc.)

    - Why to use a relational database

      - ease of use
      - ability to do JOINS
      - ability to do aggregations and analytics
      - effective for smaller data volumes
      - easier to change business requirements
      - flexibility for queries
      - modeling the data, not the queries
      - secondary indexes are available
        - want to efficiently search data by a value that is not your
          primary key.  
      - ACID Transactions - provide data integrity:
        "Properties of database transactions intended to guarantee
        validity even in the event of errors, power failures."
        
        - Atomicity
          "the whole transaction is processed or nothing is processed"
          
        - Consistency
          "only transactions that abide by constraints and rules is
          written into the database, otherwise the database keeps its
          state"
          
        - Isolation
          "transactions are processed independently and securely,
          order does not matter"
          rows lock until queries complete, so you don't get collisions.
          
        - Durability
          "Once a transaction has been COMMITED and is complete it will remain
          committed even in the event of a system failure."

    - When NOT to use a relational DB system

      - Big Data
      - Need to store different types of data formats
      - need high throughput. ACID transactions slow down the
        reading/writing of data.
      - need a flexible schema. these allow columns to be added that
        are not used for every row. Saves space.
      - need *high* *availability*. RDBS are not distributed (if they are,
        they have a coordinator/worker architechture), so in the
        event of failure, a fail-over to a backup system must occur
        which is often costly (in time).
      - Need *horizontal* *scalability*. the ability to add more machines
        or nodes to a system to increase performance and storage.
        (vertical scaling means adding more resources to an individual
        machine - RDBS can do this.)

    - Intro to Postgresql

      An open source object-relational database system.

      - builds on SQL by providing features that reliably store and
        scale complicated workloads.

      - the syntax is different from other sql syntaxes.

      - setup: https://www.codementor.io/@engineerapart/getting-started-with-postgresql-on-mac-osx-are8jcopb

        - Note: postgres doesn't like capitals. need to double quote
          anything with caps. ie. \password Yoni fails. \password
          "Yoni" succeeds.
          
* Intro to NoSql (Not Only SQL)

  - Common nosql dbs:
    - Apache Cassandra (Partition Row Store)
      data is distributed by partition and organized in column/row format
    - MongoDB (Document Store)
      easy to do search on
    - DynamoDB (Key-Value Store)
      data represented as a collection of key-value pairs
    - Apache HBase (Wide Column Store)
      also uses table:column/row format. but has a flexible schema, so
      columns can vary by row
    - Neo4j (Graph Database)
      data represented as nodes and edges

  - Basics of Cassandra

    - Keyspace
      collection of tables
    - Table
      group of partitions
    - Rows
      a single item
    
    - Partition
      - fundamental unit
      - collection of rows
      - how data is distributed
    - Primary Key
      - primary key is made up of a partition key and clustering columns
    - Columns
      - clustering and data
      - labeled element
    
    - Examples of Cassandra use cases:
      - transaction logging
      - internet of things
      - timeseries data
      - any workload that is heavy on write operations.

    - Cassandra is not good for analytics:
      GROUP BY queries are not available (?). data modeling is based
      on the desired query (!), so you can't do ad-hoc
      queries. However, you can add clustering columns into the data
      model and create new tables.

    - Cassandra uses its own query language called CQL

    - When to use NoSQL:
      - need to store different types of file formats
      - large amounts of data
      - need horizontal scalability
      - need high throughput
      - need a flexible schema
      - need high availability
      - users are distributed
        --low latency

    - When not to use NoSQL:
      - small dataset
      - need ACID transations
      - need to be able to do joins
      - want to be able to do aggregations and analytics
      - changing business requirements
      - queries are unavailable and you need the flexibility. i.e. you
        need your queries in advance.
      - (Note) Some nosql dbs do provide some form of ACID
        transations. Mongodb added the ability to do so in some
        contexts in v4.0 and v4.2. Another is MarkLogic.

    - Installing Cassandra:
      https://cassandra.apache.org/doc/latest/cassandra/getting_started/installing.html

    - Figuring this out took me 2 hours...
      - You need to run:
        - docker run --name <some name> --network <network name> -d
          cassandra:latest
        - I used network 'bridge'
      - Then make sure you have a python:3.8-slim image (which I
        needed to make a docker login to get (?)) and run:
        - docker run -it --rm --network <network name> python:3.8-slim
          bash
        - pip install cassandra-driver
      - Then within a python terminal:
        - from cassandra.cluster import Cluster
          cluster = Cluster(['172.17.0.2'], port=9042)
        - The stackoverflow answer I used told me to use the container
          name 'some-cassandra' but that didn't work. so I looked up
          some-cassandra's machines IP using:
          - docker network inspect bridge
      - I think the next step here is to create a DOCKERFILE that will
        set this up automatically.

    - Ok things are definitely working and I can make a keyspace. But
      before adding tables to the keyspace, I need to know the queries
      I'm going to make. In the demo, the query I'm going
      to run is "select * from music_library where year == 1970". That
      means 'year' needs to be my partition key and the artist name
      will be the clustering column. THERE ARE NO DUPLICATES IN APACHE
      CASSANDRA. so: PRIMARY KEY(year, artist_name)
        
* Lesson 2: Relational Data Models (7/8/22)

  - Definitions:

    - *Database*:
      a set of related data and the way it is organized.
    - *Database Management System*:
      The software that allows you to interact with a database.

  - *History*:

    - RDBs were invented in the 1969 by researchers are IBM. The lead
      researcher proposed 12 rules defining RDBs.
    
  - Rule 1 (the information rule):

    - all information in a relational database is represented
      explicitly at the logical level and in exactly one way: by
      values in tables.

  - Why is the Relational Model important?

    - standardization of the data model
    - Flexibility in adding and altering tables
    - Data Integrity
    - SQL
    - Simplicity
    - Intuitive organization

  - *OLAP*:
    - Online Analytic Processing
    - Emphasis on the Analytic
    - An OLAP-optimized DB is optimized for complex analytical and
      ad-hoc queries, including aggregations. They are optimized for
      reads.

  - *OLTP*:
    - Online Transactional Processing
    - Emphasis on the Transactional
    - An OLTP-optimized DB is optimized for less-complicated queries
      in large volumes. queries are read, insert, update, and delete.

  - Structuring The DB:
    
    - Normalization:
      - The process of structuring a relational database in accordance
        with a series of *normal forms* in order to:
      - To reduce data redundancy and increase data integrity
        
    - Denormalization:
      must be done in read-heavy workloads to improve performance

    - Purposes of Normal Forms:
      - To free the db from unwanted insertions, updates, and deletion
        dependencies (ideally only update in one place.)
      - To reduce the need for refactoring the database as new types
        of data are introduced
      - to make the relational model more informative to users
      - to make the database neutral to the query statistics.

    - First Normal Form (1NF):
      - create atomic values. Each cell has unique values. No lists or
        sets in columns.
      - be able to add data without altering tables.
      - separate different relations into different tables (customers
        and sales).
      - keep relationships between tables together with foreign keys
        
    - Second Normal Form (2NF):
      - reach 1NF
      - All columns must rely on the primary key. should not need two
        elements to get a third. 

    - Third Normal Form (3NF):
      - reach 2NF
      - No transitive dependencies. reaching a value should not
        depend on values that the desires value is not logically
        dependent on. so you shouldn't have to know an award and year
        the award was issued to look up the lead singer of the band
        the award was given to.
      - When you want to update data, you want to be able to do it in
        just one place.

    - *Denormalization* (again):
      - The process of improving the read performance of a database at
      the expense of losing some write performance by adding redundant
      copies of the data.
      - comes after normailzation.
      - requires more space.

    - *Normalization* is about increasing data integrity by reducing
      the number of copies of the data. Data that needs to be added or
      updated will be done in as few places as possible.

    - *Denormalization* is increasing the performance of reads by
      reducing the number of joines between tables. Data integrity
      will take a bit of a hit, as there will be more copies of the
      data.

    - *Fact* and *Dimension* tables
      - Work together to form an organized data model
      - Not created differently from the DDL perspective,
        they are conceptual, and important from an organizational
        standpoint.
    - Fact tables:
      consist of measurements, metrics, or facts of a business
      process. facts: "Events that have actually happened"
    - Dimension tables:
      a structure that categorizes facts and measures in order to
      enable users to answer business questions. Dimensions are often
      people, products, place, and time.
      
    - Different Schemas:
      Two of the most popular (because of simplicity) data mart
      schemas for data warehouses are:

      - Star Schema

        What:
        1. Simplest data mart schema. Consists of one or more fact
           tables referencing any number of dimension tables
        2. ERD resembles a star
        3. fact table at center with dimension tables surrounding it.
        
        Why:
        1. 3NF is a lot of work and joins can be complex.
        2. star schemas allow for relaxing of the rules (are
           denomalized) and make queries easier.
        3. aggregations perform calculations and clustering of the
           data, so the application doesn't have to.

        Cons:
        1. issues with denormalization
        2. data integrity
        3. decreased query flexibility
        4. many to many relationships need to be simplified.

      - Snowflake Schema

        What:
        1. more general class of shema, of which star is a special
           case.
        2. allows for one-to-many relationships
        3. more normalized than star, but only in 1NF or 2NF
        
* SQL NOTES

  - CREATE TABLE
    - NOT NULL
      1. ex:
         CREATE TABLE IF NOT EXISTS customers (
         customer_id int NOT NULL,
         store_id int
         );
      2. ex (composite key):
         CREATE TABLE IF NOT EXISTS customers (
         customer_id int NOT NULL,
         store_id int NOT NULL,
         spent numeric
         );
    - UNIQUE
      1. ex:
         CREATE TABLE IF NOT EXISTS customers (
         customer_id int NOT NULL UNIQUE,
         store_id int NOT NULL UNIQUE,
         spent numeric
         );
      2. ex (table constraint):
         CREATE TABLE IF NOT EXISTS customers (
         customer_id int NOT NULL,
         store_id int NOT NULL,
         spent numeric,
         UNIQUE (customer_id, store_id)
         );
    - PRIMARY KEY
      1. ex:
         CREATE TABLE IF NOT EXISTS store (
         store_id int PRIMARY KEY,
         store_location_city varchar,
         store_location_state varchar
         );
      2. ex (table constraint):
         CREATE TABLE IF NOT EXISTS customers (
         customer_id int,
         store_id int,
         spent numeric,
         PRIMARY KEY (customer_id, store_id)
         );
  - UPSERT
    *updating or inserting*
    1. ex (ON CONFLICT DO NOTHING):
       CREATE TABLE IF NOT EXISTS customer_address (
       customer_id int PRIMARY KEY,
       customer_street varchar NOT NULL,
       customer_city text NOT NULL,
       customer_state text NOT NULL
       );

       INSERT INTO customer_address (customer_id, customer_street,
       customer_city, customer_state) VALUES (432, '758 Main St.',
       'Berkeley', 'CA');

       INSERT INTO customer_address (customer_id, customer_street,
       customer_city, customer_state) VALUES (432, '758 Main St.',
       'Chicago', 'IL')
       ON CONFLICT (customer_id)
       DO NOTHING;

    2. ex (ON CONFLICT DO UPDATE):
       INSERT INTO customer_address (customer_id, customer_street)
       VALUES (432, '748 Main St.')
       ON CONFLICT (customer_id)
       DO UPDATE
       SET customer_street = EXCLUDED.customer_street;

* LESSON 3: NoSQL databases

  - Basics
  - Denormalization
  - Primary keys
  - Clustering columns
  - the WHERE clause

  - Non-relational but also Not Only SQL
  
  - When to use NOSQL
    - need high availability - system is always up
    - large amounts of data
    - need linear scalability - need to add more nodes to the system
      so the performance increases linearly
    - low latency: shorter delay before the data is transferred once
      the instruction for the transfer has been received
    - need fast reads and writes.

  - apache cassandra
    - open source
    - masterless architecture
    - highly available
    - linearly scalable
    - often used for apps

  - Node <=> Server

* Basics

  - Distributed database - resilient to node failure.
    - copies of your data may not be up to date in all locations -
      this is *Eventual Consistency*
  - Eventual Consistency:
    informally guarantees that, if no new updates are made to a given
    data item, eventually all accesses to that item will return the
    last updated value.

  - *CAP Theorem*: it is impossible for a distributed data store to
    provide more than two of the following three guarantees:

    - Consistency: every read gets the latest (and correct) piece of
      information OR returns an error.

    - Availability: Every request is received and a response is given
      (without a guarantee that the result is the most up-to-date)

    - Partition Tolerance: the system continues to function even if
      connectivity is lost between nodes

  - ACID vs CAP

* NoSQL Data Models
  Note: statements are made in the context of apache cassandra

  - Denormalization is a must (for fast reads)
  - Cassandra is optimized for fast writes
  - ALWAYS think, "queries first"
  - One table per query is a great strategy
  - cassandra does not allow for joins

* Cassandra Query Language

  - does not support:
    - JOIN
    - GROUP BY
    - Subqueries

  - Has many language drivers. We'll be using Python's


* Cassandra PRIMARY KEY

  ex:

  CREATE TABLE IF NOT EXISTS cars (year int, make text, model text,
  color text, PRIMARY KEY ((make, model), year))

  - must be unique.
  - can be simple or composite.
  - made up of the partition key and any number of optional clustering
    columns. The partition key determines the distribution of the data
    across the system. The partition key can be simple or composite.
  - it is desireable to pick a key that will evenly distribute the
    data. i.e if your key is state, the data won't be distributed
    evenly because there are so many people in california than in vermont.
  - the partition key for a given row is hashed and stored on the node
    in the system assigned that range of hashed values.


  - partition key vs clustering columns. clustering columns sort
    results within partition in ascending order.

* WHERE Clause

  - WHERE must be included
  - The Partition Key must be included in the query.
    Any Clustering Columns must be used in the order they appear in
    the Primary Key.
  - WHERE allows for fast reads.


* INTRODUCTION TO DATA WAREHOUSES

  * What is a data warehouse?
    - business perspective:
      (a) There are two business processes: opertaional (make it work!)
      and analytical processes (what's going on?).
      (b) operational databases aren't necessarily good for
      analytics.
      (c) OLTP (transaction processing) vs OLAP (analytical processing)
    - A Data Warehouse is a system that enables us to support
      analytical processes.

  * Three definitions:
    - copy of transaction data specifically structured for query and
      analysis
    - subject-oriented, integrated, nonvolatile, and time-variant
      collection of data in support of management's decisions.
    - a system that retrieves and consolidates data periodically from
      the source systems into a dimensional or normalized data
      store. it usually keeps years of history and is queried for
      business intelligence or other analystical activities. It is
      typically updated in batches, not every time a transaction
      happens in the source system.

  * Dimensional Model Review

    - Star Schema: fact and dimension tables.
      (a) Fact tables record events relevant to the business. contains
      quantifiable metrics: quantity of an item, duration of a call,
      book rating, etc. A good candidate for a fact is numeric and additive.
      (b) Dimension tables provide contexts and attributes for the
      Fact tables. item purchased, customer who made the call, etc.

  * Naive etl: moving from 3NF to Star
    - extract from 3NF
    - transfrom in the form of joins, type changes, and new columns
    - load by inserting into fact and dimension tables.

  * DWH Architectures:
    - Kimball's Bus
    - Independent Data Marts
    - Inmon's Corporate Information Factory (CIF)
    - Hybrid Bus & CIF

  * Kimball's Bus:
    - Source Transactions then:
    - ETL System then:
    - Presentation Area:
      - Dimensional
      - Atomic and summary Data
      - organized by business processes
      - uses conformed dimensions (generalize dimensions so that all
        dimensions are usable by the whole organization)
    - Results in a bus Matrix (where things are)
    - ETL:
      - extracting
        - get data from source
        - possibly deleting old state
      - transforming
        - integrates many sources together
        - possible cleaning
        - possible meta diagnostics
      - Loading
        - structuring and loading into a dimensional model

  * Independent Data Marts
    - Each department has separate ETL processes
    - there is work and data repitition
    - can give uncoordinated and inconsistent views
    - the advantage is departmental autonomy, avoids coordination
      overhead.

  * Inmon's Corporate Information Factory (CIF)
    - Data acquisition then:
    - (ETL) enterprise dataware house (3nf Database) then:
      - single integrated source of truth for data marts
      - can be accessed by end-users if necessary
    - (ETL) data delivery to:
    - data marts (now have a 3nf baseline)
      - dimensionally modelled and (unlike Kimball's) mostly
        aggregated.

  * Hybrid Kimball Bus & Inmon CIF
    - Removes Data Marts
    - Exposes the enterpries DW bus architecture

  * OLAP Cubes

    - From a star schema, create OLAP cubes
    - These are tables aggregating a number of facts/dimensions. For example
      Tables for Movie, Revenue, Branch/Location, Month (1 table for each month)
    - These are very easy for business users

    - *Rolling Up and Drilling Down*:
      - aggregating and disaggregating on specific dimensions. I.e sum
        of sales for each country (summing over city). or decomposing
        cities into smaller districts.

    - OLAP cubes should store data at the finest granularity.

    - *Slicing and Dicing*:
      - slices are essentially marginal distributions.
      - dices are sub-cubes created by restricting the range of
        certain values.

    - OLAP Cubes Query Optimization
      - There is a GROUP by CUBING statements:
        - GROUP BY SETS
        - GROUP BY CUBE
      - When you call CUBE, you do one pass through the facts
        table. This gives you all combinations of, for example,
        (movie, branch, month).
      - Saving all the combinations, is usually sufficient for a very
        wide number of future queries.

    - Links:
      - https://www.amazon.com/Data-Warehouse-Toolkit-Complete-Dimensional/dp/0471200247

      - https://www.amazon.com/Building-Data-Warehouse-W-Inmon/dp/0764599445

      - https://www.amazon.com/Building-Data-Warehouse-Examples-Experts/dp/1590599314

    - OLAP Cube Technologies:
      - MOLAP (multidimensional online analytical processing)
        - cubes are stored on a dedicated server using a
          non-relational database structure.
      - ROLAP (Relational online analytical processing)
        - cubes are created on the fly and stored in relational
          databases

* AWS

  - Create an IAM role
    - select entity (AWS service)
    - select use case (Redshift - Customizable)
    - Attach permissions policies: check AmazonS3ReadOnlyAccess
    - add role name (MyRedshiftRole)
      
  - Create Security Group
    - Navigate to EC2 services
    - open security groups
    - create new security group
      - group name: redshift_security_group
      - description: Authorize Redshift Cluster Access
      - VPC: default
      - Inbound rules:
        - add new:
          - Type: Custom TCP
          - Protocol: TCP
          - Port Range: 5439 (Default)
          - source: 0.0.0.0/0 (anywhere in the world)
            (note: for demonstration purposes only. this allows access
            from any computer on the internet!)
      - Outbound rules:
        - default (allow traffic to anywhere)
          
  - Launch a Redshift Cluster
    - Create Cluster:
      - unique id: redshift-cluster-1
      - choose free trial.
      - cluster specs (default):
        - dc2.large
        - 2 vCPUs
        - 160 GB storage
      - username and pw created and stored.
    - (Note: in free tier the following can be configured once the
      cluster is available)
    - Associate IAM role MyRedshiftRole
    - Network and Security:
      - VPC: default
      - VPC Security Groups: redshift-security-group (and default?)
    - Subnet Groups
      - create cluster subnet group and select in network/security
        configuration (I wasn't able to do this...)
        
  - Create IAM User
    - give username
    - allow programmatic access
    - attach existing policies:
      - RedshiftFullAccess
      - S3ReadOnly
    - review and download credentials (.csv)
      
  - S3
    - (unique) name
    - allow public access
    - disable versioning and encryption
      
  - Amazon RDS
    - create database
    - postgresql - latest version is default
    - free tier template
    - postgreSQL-test identifier
    - master username and password stored
    - default storage
    - default connectivity with:
      - public access YES (choose no when more familiar)
      - redshift_security_group and default
    - provide a db name in additional configuration (testdb)

* DWH on AWS
  
  - Cloud Managed:
    - Amazon RDS, DynamoDB, S3
    - re-use of expertise, less staff, etc.
    - deal with complexity through 'infrastructure as code'

  - Self Managed:
    - EC2 + {postgresql, cassandra, unix}
    - essentially have ec2 do everything

  - Redshift:
    
    - column-oriented storage
    - best oriented for OLAP workloads.
    - internally, it's a modified postgresql

    - most relational databases execute queries in parallel, with each
      cpu executing a single query. This is good for OLTP, as you may
      have many users executing small updates and retrievals.

    - Massively parallel processing (MPP) parallelizes the execution
      of a single query on multiple cpus/machines.
    - This is done via table partitioning.

    - Redshift is cloud-managed, column-oriented, and MPP.

    - Architecture:
      - Leader Node:
        - coordinates compute nodes
        - handles external communication
        - optimizes query execution
      - Compute Nodes
        - each has it's own cpu, memory, and disk
        - these can be scaled up or scaled out
        - each node is divided into a number of slices: a cluster with
          n slices can process n partitions of a table simultaneously.
          - a slice has atleast one cpu and dedicated storage and memory
          - sum of all slices across all compute nodes is the unit of
            parallelization.
      - Nodes can be optimized for compute or storage. these come with
        cost/benefit pros/cons.
      - The coming demo with be 4 nodes at $.25/hour! *SET UP BILLING ALERTS*

  - SQL-TO-SQL ETL
    
    - we will want to copy the results of a query to another table on
      a totally different database server.
    - this is feasible when both db servers are running the same
      RDBMS, but much harder if different.
    - Either way, there will likely be some transforming, cleaning,
      and or governance that needs to be done along the way.

    - with an EC2 instance we can direct one db to transfer (COPY) to s3, and
      then another db to query that s3 bucket. This bypasses the need
      for local storage on EC2.

    - FLOW IN CONTEXT:
      - sources:
        - s3
        - cassandra
        - ec2
        - dynamodb
        - etc.
      - ETL:
        - ec2
        - aws data pipeline
        - airflow
      - STAGING:
        -s3
      - DWH:
        - redshift
      - OLAP CUBES:
        - s3
        - aws rds
        - cassandra
      - BI Apps
        - jupyter
        - tableau

    - Transferring data from s3 staging to redshift
      - use the COPY command
      - may be better to break into multiple files and ingest in
        parallel:
        - use a common prefix
        - a manifest file
      - better to ingest from the same aws region
      - better to compress all the csv files

    - COPY (PREFIX) example
      #+BEGIN_SRC sql
        COPY sporting_event_ticket FROM 's3://udacity-labs/ticket/split/part'
        CREDENTIALS 'aws_iam_role=arn:aws:iam::464956546:role/dwhRole'
        gzip DELIMETER ';' REGION 'us-west-2';
      #+END_SRC
      'part' can just be a prefix matching multiple files.

    - COPY (MANIFEST) example:
      #+BEGIN_SRC sql
      COPY customer
      FROM 's3://mybucket/cust.manifest'
      IAM_ROLE 'arn:aws:iam::01234567890:role/myRedshiftRole'
      manifest;
      #+END_SRC sql
      with cust.manifest:
      {
      'entries': [
      {"url": "s3://mybucket-alpha/2013-10-04-custdata", "mandatory" :
      true},
      {"url": "s3://mybucket-alpha/2013-10-05-custdata", "mandatory" :
      true},
      {"url": "s3://mybucket-alpha/2013-10-06-custdata", "mandatory" :
      true}
      ]
      }

    - compression
      - redshift gives control over each column's compression. (see
        redshift documentation)
      - COPY makes a best effort to compress each column optimally

    - ETL from other sources
      - possible to ingest directly from ssh from an EC2 machine's
        storage
      - otherwise:
        - s3 is a *staging area*
        - so some other EC2 ETL working needs to run the ingestion
          jobs via some *orchestarted dataflow product* like airflow,
          luigi, nifi, streamset, or aws data pipeline.

    - ETL *OUT* of redshift
      - redshift can connect directly to BI Apps via (JDBC/ODBC)
      - may want to extract pre-aggregated OLAP cubes:
        #+BEGIN_SRC sql
        UNLOAD ('select * from venue limit 10')
        to 's3://mybucket/venue_pipe_'
        iam_role 'arn:aws:iam:012345:role/MyRedshiftRole';
        #+END_SRC sql
        (in this case read credentials are required)

  - Building a Redshift Cluster
    (see above, there is no longer a quick-launch option. Plus I don't
    think this example is explicit enough: the free-trial version
    doesn't allow you to change the number of nodes. If it does, it
    must be post startup.)

  - Issues:
    - the previous cluster is operational, but we need additional
      functionality:
      - Security:
        - needs to be accessible *ONLY* from the VPC
        - need to access it from our jupyter workspace.

      - Access to s3:
        - the cluster needs (read) access to an s3 bucket.

    - Redshift needs an IAM role.
    - Chage the TCP ports open in the security group

  - Modern engineering uses *Infrastructure as Code*
    
    - "creating a machine is as easy as opening a file"

    - aws-cli
    - aws sdk (python, java, etc.)
    - amazon cloud formation (jscon description)
      - atomic (everything succeeds or nothing does)

    - we will proceed with aws sdk for python: boto3
    - we'll create an IAM user dwhadmin and give it admin privileges
      (this is, in general excessive, but helpful for the demo)
    - user gets an access token and secret.
      
      
      
